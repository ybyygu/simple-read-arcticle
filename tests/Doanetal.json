{"doc_name":"Doan et al","pages":[{"id":0,"width":607.408,"height":800.901,"need_ocr":false},{"id":1,"width":607.408,"height":800.901,"need_ocr":false},{"id":2,"width":607.408,"height":800.901,"need_ocr":false},{"id":3,"width":607.408,"height":800.901,"need_ocr":false},{"id":4,"width":607.408,"height":800.901,"need_ocr":false},{"id":5,"width":607.408,"height":800.901,"need_ocr":false},{"id":6,"width":607.408,"height":800.901,"need_ocr":false},{"id":7,"width":607.408,"height":800.901,"need_ocr":false},{"id":8,"width":607.408,"height":800.901,"need_ocr":false},{"id":9,"width":607.408,"height":800.901,"need_ocr":false},{"id":10,"width":607.408,"height":800.901,"need_ocr":false},{"id":11,"width":607.408,"height":800.901,"need_ocr":false}],"blocks":[{"id":0,"kind":{"block_type":"Header","text":"Downloaded via UNIV OF CHINESE ACADEMY OF SCIENCES on March 11, 2025 at 09:25:38 (UTC). See https://pubs.acs.org/sharingguidelines for options on how to legitimately share published articles."},"pages_id":[0],"bbox":{"x0":1.4059908,"y0":232.9329,"x1":19.76646,"y1":568.7378}},{"id":1,"kind":{"block_type":"TextBlock","text":""},"pages_id":[0],"bbox":{"x0":51.04679,"y0":84.8388,"x1":105.60669,"y1":93.491394}},{"id":2,"kind":{"block_type":"TextBlock","text":""},"pages_id":[0],"bbox":{"x0":61.63753,"y0":193.74681,"x1":233.77237,"y1":202.3438}},{"id":3,"kind":{"block_type":"Image","id":0,"caption":null},"pages_id":[0],"bbox":{"x0":50.16824,"y0":27.020794,"x1":206.34073,"y1":58.411613}},{"id":4,"kind":{"block_type":"Title","level":0,"text":"Fcg-Former: Identification of Functional Groups in FTIR Spectra Using Enhanced Transformer-Based Model"},"pages_id":[0],"bbox":{"x0":51.952633,"y0":109.6502,"x1":529.1354,"y1":146.26419}},{"id":5,"kind":{"block_type":"TextBlock","text":"Vu Hoang Minh Doan,# Cao Duong Ly,# Sudip Mondal,# Thi Thuy Truong, Tan Dung Nguyen, Jaeyeop Choi, Byeongil Lee,* and Junghwan Oh*"},"pages_id":[0],"bbox":{"x0":51.09718,"y0":149.80132,"x1":537.9955,"y1":179.33875}},{"id":6,"kind":{"block_type":"Header","text":""},"pages_id":[0],"bbox":{"x0":526.74493,"y0":86.416794,"x1":550.60815,"y1":92.90758}},{"id":7,"kind":{"block_type":"Image","id":1,"caption":null},"pages_id":[0],"bbox":{"x0":337.97946,"y0":257.43604,"x1":472.2257,"y1":388.28403}},{"id":8,"kind":{"block_type":"TextBlock","text":"ABSTRACT: Deep learning (DL) is becoming more popular as a useful tool in various scientific domains, especially in chemistry applications. In the infrared spectroscopy field, where identifying functional groups in unknown compounds poses a significant challenge, there is a growing need for innovative approaches to streamline and enhance analysis processes. This study introduces a transformative approach leveraging a DL methodology based on transformer attention models. With a data set containing approximately 8677 spectra, our model utilizes self-attention mechanisms to capture complex spectral features and precisely predict 17 functional groups, outperforming conventional architectures in both functional group prediction accuracy and compound-level precision. The success of our approach underscores the potential of transformer-based methodologies in enhancing spectral analysis techniques."},"pages_id":[0],"bbox":{"x0":51.338303,"y0":393.5946,"x1":555.9495,"y1":479.37875}},{"id":9,"kind":{"block_type":"Title","level":3,"text":" INTRODUCTION"},"pages_id":[0],"bbox":{"x0":66.84377,"y0":505.3765,"x1":139.72092,"y1":514.3219}},{"id":10,"kind":{"block_type":"TextBlock","text":"Functional groups serve to identify the physical properties including boiling point,1 melting point,2 solubility,3 and viscosity4 of chemical compounds. By recognizing functional groups, researchers can classify and categorize compounds, aiding in their characterization and identification.5 In the field of medicinal chemistry, functional groups play a critical role in determining a compound's biological activity and pharmacological properties.6 Specific functional groups might impart desirable therapeutic effects or influence the compound's interaction with biological targets.7 Understanding these relationships is essential for drug design and optimization. In polymer chemistry, by controlling the types and distribution of functional groups, researchers can tailor their mechanical, thermal, and chemical properties for specific applications, such as in materials science, engineering, and biomedicine.8−10 Knowledge of functional groups is thus essential for understanding and predicting the behavior of substances in various environments."},"pages_id":[0],"bbox":{"x0":51.21046,"y0":521.1432,"x1":292.4839,"y1":725.38086}},{"id":11,"kind":{"block_type":"Image","id":2,"caption":null},"pages_id":[0],"bbox":{"x0":51.61945,"y0":758.96094,"x1":166.10149,"y1":782.46497}},{"id":12,"kind":{"block_type":"TextBlock","text":"Functional groups are specific arrangements of atoms within a molecule that give the compound its unique chemical"},"pages_id":[0],"bbox":{"x0":51.414967,"y0":727.69147,"x1":292.29193,"y1":748.77936}},{"id":13,"kind":{"block_type":"TextBlock","text":"\ncharacteristics.11 Infrared (IR) spectroscopy is a widely used qualitative and quantitative analytical method utilized for the identification and characterization of chemical compounds, relying on their molecular vibrations.12 When a sample is exposed to infrared radiation, certain wavelengths are selectively absorbed by its chemical bonds, causing transitions between quantized vibrational energy levels.13 Even at the lowest energy state, known as the zero point energy, molecular bonds possess intrinsic vibrational energy.14 The absorption of infrared radiation provides the exact amount of energy required to elevate the molecule from a lower vibrational state to a higher one.15 Each type of chemical bond has a characteristic vibrational frequency associated with it, corresponding to the energy difference between these quantized"},"pages_id":[0],"bbox":{"x0":315.36795,"y0":504.09988,"x1":556.5393,"y1":664.2998}},{"id":14,"kind":{"block_type":"Footer","text":""},"pages_id":[0],"bbox":{"x0":293.16364,"y0":768.8892,"x1":313.42484,"y1":775.8822}},{"id":15,"kind":{"block_type":"TextBlock","text":"Received: March 28, 2024 Revised: June 28, 2024 Accepted: June 28, 2024 Published: July 15, 2024"},"pages_id":[0],"bbox":{"x0":315.1743,"y0":681.53326,"x1":414.88712,"y1":722.53357}},{"id":16,"kind":{"block_type":"Footer","text":"©"},"pages_id":[0],"bbox":{"x0":186.80205,"y0":763.24817,"x1":280.9564,"y1":770.5033}},{"id":17,"kind":{"block_type":"Image","id":3,"caption":null},"pages_id":[0],"bbox":{"x0":513.34607,"y0":674.2944,"x1":556.3248,"y1":735.7846}},{"id":18,"kind":{"block_type":"Footer","text":"\nhttps://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369"},"pages_id":[0],"bbox":{"x0":439.35297,"y0":762.52826,"x1":555.72266,"y1":774.8031}},{"id":19,"kind":{"block_type":"Image","id":4,"caption":null},"pages_id":[1],"bbox":{"x0":113.100685,"y0":50.2781,"x1":492.1318,"y1":558.36646}},{"id":20,"kind":{"block_type":"Header","text":""},"pages_id":[1],"bbox":{"x0":527.3648,"y0":42.02474,"x1":549.5125,"y1":48.522255}},{"id":21,"kind":{"block_type":"TextBlock","text":""},"pages_id":[1],"bbox":{"x0":303.41223,"y0":39.752937,"x1":359.13498,"y1":48.9951}},{"id":22,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[1],"bbox":{"x0":51.6564,"y0":37.83231,"x1":142.20364,"y1":49.627308}},{"id":23,"kind":{"block_type":"TextBlock","text":"levels.16 This characteristic absorption allows IR spectroscopy to detect and analyze the structural features of organic compounds effectively.15"},"pages_id":[1],"bbox":{"x0":51.257076,"y0":624.1074,"x1":292.7091,"y1":658.91736}},{"id":24,"kind":{"block_type":"TextBlock","text":"The IR spectra produced from IR spectroscopy are typically investigated by manually marking all relevant peaks corresponding to specific functional groups. Even with the expertise of researchers and the assistance of available documents, this manual procedure is time-consuming, especially in the case of complex compounds. Moreover, various physical and chemical factors that affect the sample's constituents may cause changes in the structural environment of specific functional groups,"},"pages_id":[1],"bbox":{"x0":51.261326,"y0":660.3695,"x1":292.22397,"y1":753.96454}},{"id":25,"kind":{"block_type":"TextBlock","text":"\nleading to notable deviations in those groups' typical peak frequencies from their representative ranges.17 There might be an overlap phenomenon in the fingerprint region (400−1500 cm−1 \n) of most IR spectra. Because each material's distinctive properties are contained in this frequency band, it becomes much more difficult for researchers to identify specific peaks and corresponding functional groups. To reduce the inefficiency of manual interpretation, the computer and AIaided approach such as innovative deep learning (DL) is suggested when analyzing the IR spectra."},"pages_id":[1],"bbox":{"x0":315.23645,"y0":624.50885,"x1":557.4819,"y1":754.2324}},{"id":26,"kind":{"block_type":"TextBlock","text":"\nFigure 1. Distribution of (A) training data set, (B) validation data set, and (C) test data set."},"pages_id":[1],"bbox":{"x0":51.581463,"y0":603.6887,"x1":379.9902,"y1":613.0044}},{"id":27,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12359"},"pages_id":[1],"bbox":{"x0":293.4074,"y0":765.2658,"x1":555.6206,"y1":777.8483}},{"id":28,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[2],"bbox":{"x0":51.634632,"y0":37.90735,"x1":142.53703,"y1":49.644417}},{"id":29,"kind":{"block_type":"TextBlock","text":"DL is a subset of machine learning that harnesses neural networks to process data like human brains. These computer models are trained to extract features from raw data and make predictions and classifications. DL models can be trained in chemistry to predict the presence or absence of specific functional groups in chemical compounds based on IR spectra.18 This is particularly useful when dealing with complex spectra with multiple overlapping peaks.19 Recently, several studies were reported using DL models to investigate the IR spectra. The most successful and efficient model presented in this field is the convolutional neuron network (CNN) or CNN-based model,18−27 recognized for its remarkable efficacy in predicting functional groups. However, evaluating these models solely based on functional group prediction precision may lead to an incomplete assessment. It is imperative to also consider the accuracy of the model in predicting entire molecules. This consideration arises from data imbalance issues, where certain functional groups are more prevalent than others within the data set. To illustrate, if a molecule comprises five functional groups and a model accurately predicts four out of the five, conventional accuracy metrics may suggest an 80% success rate. However, from the perspective of predicting the entire molecule, the accuracy would be 0%. This underscores the need for a more comprehensive evaluation framework encompassing functional group prediction precision and compound-level accuracy."},"pages_id":[2],"bbox":{"x0":51.305363,"y0":331.3697,"x1":292.30048,"y1":619.81616}},{"id":30,"kind":{"block_type":"TextBlock","text":"In recent years, transformer models have emerged as the cornerstone of various machine learning applications, revolutionizing the field with their remarkable capabilities in handling diverse data types such as signals, images, speech, and text.28 The transformer architecture, initially introduced for natural language processing tasks,29 has showcased exceptional performance across various domains, including translation,30,31 time series forecasting,32−34 and signal classification.35−39 Overview of the existing landscape of machine learning models for functional group characterization, traditional models, such as 1D-CNNs, recurrent neural networks (RNNs),40 and long short-term memory,41 have historically dominated functional"},"pages_id":[2],"bbox":{"x0":51.2656,"y0":622.4487,"x1":292.80344,"y1":753.9729}},{"id":31,"kind":{"block_type":"TextBlock","text":"\ngroup analysis tasks.19,27,36,42−47 Despite these achievements, there remains a noticeable gap in the literature regarding the application of transformer models to chemical spectra signals, particularly in the functional group characterization data sets."},"pages_id":[2],"bbox":{"x0":315.0474,"y0":328.4159,"x1":555.99994,"y1":372.9261}},{"id":32,"kind":{"block_type":"TextBlock","text":"To explore the potential benefits and challenges associated with adopting transformer approaches in this domain, an attention-based transformer model was utilized for predicting the function groups within IR spectra. The architecture of the transformer model, encompassing 17 multilabel functional groups as inputs, is depicted in Figure 1. The model's performance was evaluated by assessing both the accuracy of functional group predictions and the precision of compoundlevel predictions."},"pages_id":[2],"bbox":{"x0":315.32703,"y0":373.10083,"x1":556.34393,"y1":472.2464}},{"id":33,"kind":{"block_type":"Title","level":2,"text":" MATERIALS AND METHODS"},"pages_id":[2],"bbox":{"x0":329.32803,"y0":485.20102,"x1":459.01917,"y1":496.83466}},{"id":34,"kind":{"block_type":"TextBlock","text":"Data Collection and Functional Groups Assignment. We obtained the FTIR absorbance spectra for all compounds from the National Institute of Standards and Technology (NIST) Chemistry WebBook.48 These spectra were initially downloaded in the JDX format and subsequently converted to XY files. Finally, all converted spectra were consolidated and stored in a single CSV file, as per the specifications outlined in a species file. We match the ID of each compound to the IUPAC InChi strings by using the PubChem API.49 Substructure matching was afterward carried out by RDKit on each string to determine whether a predetermined compound topology was present.50 Each SMARTS string was tested independently and if a match was found, the functional group was classified as belonging to the corresponding compound. Another group of spectra�an external data set�including 17 spectra was downloaded from the Emission Measurement Center Spectral Database. All spectra were processed the same way as those from the NIST Chemistry WebBook."},"pages_id":[2],"bbox":{"x0":315.21912,"y0":503.03888,"x1":557.0554,"y1":709.9848}},{"id":35,"kind":{"block_type":"TextBlock","text":"Framework. Figure 2 depicts the comprehensive FITR functional groups classification flowchart, encompassing the preparation of input spectra, data segmentation into functional groups within a single molecule, training-validation-testing data"},"pages_id":[2],"bbox":{"x0":315.29187,"y0":712.01917,"x1":556.21295,"y1":753.9913}},{"id":36,"kind":{"block_type":"Image","id":5,"caption":null},"pages_id":[2],"bbox":{"x0":83.318794,"y0":47.872993,"x1":520.6342,"y1":290.45633}},{"id":37,"kind":{"block_type":"TextBlock","text":"\nFigure 2. Process for classifying FTIR spectra using two models, one based on convolutional neural network (IRCNN) and the other on transformer architecture."},"pages_id":[2],"bbox":{"x0":51.513496,"y0":299.46786,"x1":553.78937,"y1":318.8421}},{"id":38,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12360"},"pages_id":[2],"bbox":{"x0":293.35553,"y0":765.49634,"x1":555.46735,"y1":777.9014}},{"id":39,"kind":{"block_type":"TextBlock","text":"splicing, utilization of a DL network, optimization of hyperparameters, model comparison, postprocessing, and the deployment of the model. First, the spectra, initially presented in XY format within a CSV file, underwent conversion into a unified 1D-array data set in NumPy format. Each data was then paired with a corresponding text annotation file. To ensure uniformity, all spectra were standardized to a consistent set of feature points (3600 points spanning the range from 400 to 4000 cm−1 \n) by employing the linear interpolation method, as defined in eq 1. The spectra data concerning the number of functional groups that existed in each molecule were also investigated. The maximum number of functional groups in a single molecule is seven, and the distribution of each group reveals an imbalance, as illustrated in Figure S1A−C. Spectra were then segmented into seven groups to address this imbalance. This method ensures that models are trained and evaluated on data sets accurately representing the distribution of functional groups, thereby promoting a more balanced data set regarding the number of functional groups per molecule.51 Subsequently, all groups were randomly partitioned into training, validation, and test sets with allocation ratios of 75, 15, and 10%, respectively. During the training process, input spectra values were scaled to a range of 0 to 1 using the min− max normalization method, ensuring consistent data scales."},"pages_id":[3],"bbox":{"x0":51.366753,"y0":306.71756,"x1":292.16336,"y1":567.8127}},{"id":40,"kind":{"block_type":"TextBlock","text":"We conducted a multilabel classification with 17 classes of functional groups, assessing the performance of both the CNN-based model27 (specifically IRCNN, a published model) and our proposed transformer-based model based on the test set. The external data set was used to further evaluate our model's reliability. Two classical machine learning classifiers: decision trees (DTs) and K-nearest neighbors (KNN) were also used to compare with our proposed model."},"pages_id":[3],"bbox":{"x0":51.249725,"y0":569.6595,"x1":292.06155,"y1":655.97894}},{"id":41,"kind":{"block_type":"TextBlock","text":"y y x x y y x x \n( )( ) 1 \n1 1 2 2 1 \n=  + (1)"},"pages_id":[3],"bbox":{"x0":63.96881,"y0":663.5676,"x1":291.45203,"y1":691.57794}},{"id":42,"kind":{"block_type":"TextBlock","text":"where x1, y1 represent the coordinates of the left position, x2, y2 represent the coordinates of the right position, x is the interpolation point and y denotes the interpolated value."},"pages_id":[3],"bbox":{"x0":50.933178,"y0":701.18774,"x1":291.5077,"y1":731.8608}},{"id":43,"kind":{"block_type":"TextBlock","text":"DL Model. Irchracterization Convolutional Neural Networks. The irchracterization convolutional neural networks"},"pages_id":[3],"bbox":{"x0":51.65924,"y0":733.52124,"x1":291.0016,"y1":753.6759}},{"id":44,"kind":{"block_type":"Image","id":6,"caption":" "},"pages_id":[3],"bbox":{"x0":86.675446,"y0":49.308456,"x1":534.5356,"y1":276.89606}},{"id":45,"kind":{"block_type":"Header","text":""},"pages_id":[3],"bbox":{"x0":527.7032,"y0":41.64602,"x1":549.3351,"y1":48.704407}},{"id":46,"kind":{"block_type":"TextBlock","text":"\n(IRCNN)27 is a DL method proposed for identifying functional groups within organic molecules. Unlike other approaches that utilize artificial neural networks, IRCNN employs sliding convolutional filters with a shared-weight architecture across input features, resulting in translational equivariant responses referred to as feature maps. In this research, we chose to reimplement the IRCNN model, which offers a novel spectral interpretation approach. The IRCNN architecture is composed of two convolution blocks, one flattened layer, three dense layers, and one activation layer. We utilized PyTorch to carry out the reimplementation while maintaining all parameters consistent with the original paper, as detailed in both the paper itself and a provided GitHub link at https://github.com/gj475/irchracterizationcnn. Each convolution block consists of a dense convolution layer, batch normalization, ReLU activation, and a max-pooling layer. In the activation layer, unlike typical classification tasks, the spectra signal corresponds to multiple class labels. Consequently, the authors opted for sigmoid activation instead of softmax activation to accommodate these multilabel class labels. The IRCNN architecture is shown in Figure 3."},"pages_id":[3],"bbox":{"x0":315.28513,"y0":303.46866,"x1":556.2839,"y1":540.37976}},{"id":47,"kind":{"block_type":"TextBlock","text":"Transformer Architecture. We propose an approach that directly adapts the full transformer architecture,29 initially designed as a neural machine translation model. This model is particularly effective for handling sequential data through the aiding of attention mechanisms.52 The transformer network utilizes fundamental concepts of an encoder-decoder architecture, with each block incorporating simple word embeddings, attention mechanisms, and softmax. It avoids the structural complexities present in RNNs or CNNs. The encoder extracts features from an input sequence, while the decoder utilizes these features to generate an output sequence. In this study, our primary objective is spectra classification. Therefore, we opted to implement only the encoder part, which is designed to learn embeddings suitable for the efficient classification task. The encoder component in transformer architecture plays a vital role in comprehending and extracting relevant information from the input sequence. Over the years, numerous encoder-only architectures have been utilized, drawing inspiration from the encoder module of the original"},"pages_id":[3],"bbox":{"x0":315.49402,"y0":541.9926,"x1":556.31995,"y1":753.9231}},{"id":48,"kind":{"block_type":"Header","text":""},"pages_id":[3],"bbox":{"x0":51.61123,"y0":37.927868,"x1":142.81085,"y1":49.548695}},{"id":49,"kind":{"block_type":"Title","level":3,"text":"\nFigure 3. IRCNN architecture."},"pages_id":[3],"bbox":{"x0":50.89592,"y0":283.80573,"x1":160.40587,"y1":293.06967}},{"id":50,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12361"},"pages_id":[3],"bbox":{"x0":293.27563,"y0":765.5,"x1":555.4389,"y1":777.9856}},{"id":51,"kind":{"block_type":"Header","text":""},"pages_id":[4],"bbox":{"x0":51.608685,"y0":37.900505,"x1":142.80138,"y1":49.537174}},{"id":52,"kind":{"block_type":"TextBlock","text":"transformer model. Examples include BERT (bidirectional encoder representations from transformers),53 RoBERTa (a Robustly optimized BERT pretraining approach),54 and ViT (vision transformer).55 These architectures have been instrumental in advancing various classification domains. Despite the transformer architecture's emergence as state-ofthe-art (SOTA) for natural language processing and vision tasks, its application to chemical signals has remained limited. In this study, we introduced a transformer model (Fcg-former) (Figure 4A) inspired by the transformer encoder architecture and the ViT model. By adapting this architecture to process sequences of signal patches, our approach has shown outstanding performance on spectra classification tasks. Leveraging attention mechanisms, Fcg-former achieves remarkable results compared to SOTA convolutional networks trained on the same resources. The overall architecture of Fcgformer is shown in Figure 4."},"pages_id":[4],"bbox":{"x0":51.301163,"y0":379.8068,"x1":292.0158,"y1":566.0797}},{"id":53,"kind":{"block_type":"TextBlock","text":"Self-attention (Figure 4B) is a crucial mechanism utilized in transformer encoders, enabling the model to focus on different parts of the input sequence when processing each element- (token). The self-attention mechanism generates three versions of the input embeddings: queries, keys, and values. These are linear projections of the original embeddings and are used to calculate attention scores. Attention scores, representing the importance or relevance of each patch sequence to the current patch, are computed by taking the dot product of a query with the keys. The softmax function is then applied to the attention scores to convert them into probabilities, ensuring that the attention weights sum up to 1 and indicating the relative importance of each element signal. Following the softmax operation, the model calculates a weighted average of the value vectors associated with all patch sequences. These weights are determined by the softmax probabilities obtained earlier, ensuring that patches deemed more pertinent to the"},"pages_id":[4],"bbox":{"x0":51.307175,"y0":568.45435,"x1":292.0367,"y1":753.715}},{"id":54,"kind":{"block_type":"Header","text":""},"pages_id":[4],"bbox":{"x0":527.36487,"y0":42.062847,"x1":549.4397,"y1":48.35537}},{"id":55,"kind":{"block_type":"TextBlock","text":"\ncurrent signal contribute more significantly to the final output. The resulting vector represents a signal-aware representation of the current patch, considering its relationship with other patch signals in the sequence. By applying self-attention to the spectra signal, the transformer model can capture dependencies between different patches in the input sequence and learn to focus on the most relevant patches for each position, which aids in understanding the signal and improves classification accuracy."},"pages_id":[4],"bbox":{"x0":315.2427,"y0":379.63315,"x1":555.9226,"y1":478.35492}},{"id":56,"kind":{"block_type":"TextBlock","text":"Q K V QK d \nAttention( , , ) softmax V \nT k \n= \ni k \njjjjjj \ny { \nzzzzzz (2)"},"pages_id":[4],"bbox":{"x0":326.9325,"y0":484.87656,"x1":556.12726,"y1":517.4372}},{"id":57,"kind":{"block_type":"TextBlock","text":"To effectively process the spectral signal data while ensuring consistent input size and facilitating tokenization for subsequent processing, each signal is first resized to a fixed signal length of 1024. Subsequently, the signal is divided into a sequence of fixed-size non-overlapping patches. These patches are then linearly embedded into tokens, which serve as the input to the Fcg-former model. Like BERT and ViT architectures, an additional learnable token known as the [class] token is introduced to act as the representation of the entire input signal. This token is utilized to capture global information and understanding of the signal, which proves beneficial for various tasks such as classification. It typically serves as the input to the classification head located at the output of the transformer encoder block. Each token, including an additional special token [class], is assigned learnable position embeddings. These position embeddings play an essential role in transformer-based architecture, which encodes the positional information on each token within the sequence by a unique sinusoidal extrapolability, allowing the model to understand the relative positions of tokens. However, static (not trained) value does not always perform well, due to the"},"pages_id":[4],"bbox":{"x0":315.40985,"y0":524.9945,"x1":555.9919,"y1":753.81744}},{"id":58,"kind":{"block_type":"Header","text":""},"pages_id":[4],"bbox":{"x0":303.52975,"y0":40.238724,"x1":358.76337,"y1":48.77748}},{"id":59,"kind":{"block_type":"Image","id":7,"caption":null},"pages_id":[4],"bbox":{"x0":98.73583,"y0":58.012733,"x1":506.09552,"y1":340.90118}},{"id":60,"kind":{"block_type":"TextBlock","text":"\nFigure 4. Transformer-based model: (A) Fcg-former architecture; (B) transformer encoder block with the self-attention mechanism; (C) classification head."},"pages_id":[4],"bbox":{"x0":51.795864,"y0":348.86746,"x1":554.21436,"y1":368.14536}},{"id":61,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12362"},"pages_id":[4],"bbox":{"x0":293.28568,"y0":765.467,"x1":555.5887,"y1":777.9634}},{"id":62,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[5],"bbox":{"x0":51.585033,"y0":38.564873,"x1":141.7543,"y1":49.604794}},{"id":63,"kind":{"block_type":"TextBlock","text":"lack of learnability and flexibility,56 most pretrained language models57 utilize learnable (trainable parameters) vector embedding. Subsequently, the sequence of vectors, comprising both the token embeddings and their corresponding position embeddings, is input into a transformer encoder. This encoder processes the input sequence, leveraging self-attention mechanisms to capture dependencies between patches and generate representations for each token in the sequence. A classification head is responsible for producing the final classification output, the class token (*) plays a crucial role in this process (Figure 4C). This additional learnable token is appended to the input sequence and does not correspond to any specific patch of spectrum in the input. Each layer of the transformer encoder processes the tokens, updating their representations. The class token is initialized as a fixed-size and learnable vector, which matches the embedding dimensions of the model. After passing through the final layer of the transformer encoder, the class token holds a comprehensive representation of the entire input sequence. This representation is then used by the classification head to produce the final output. Hyperparameter tuning was conducted using Neural Network Intelligence,58 aiming to optimize parameters within a transformer architecture, specifically focusing on patch size, layer count, and attention head count. This process goal was to identify the optimal configurations for these parameters that would lead to improved performance or efficiency in the given task or model architecture. The top-performing model, characterized by a signal size of 1024, patch size of 16, 2 layers, an embedded dimension of 768, and 4 attention heads, was chosen for evaluation on an independent test data set."},"pages_id":[5],"bbox":{"x0":51.039368,"y0":54.184883,"x1":292.2454,"y1":388.1575}},{"id":64,"kind":{"block_type":"TextBlock","text":"Training Methods. Both IRCNN and Fcg-former utilized the same training parameters during the evaluation of their performance. This consistency ensures a fair comparison between the two models, as they are trained under similar conditions, allowing for a more accurate assessment of their relative effectiveness in handling the given task or data set. The learning rate was set to 0.002, employing the Adam optimizer algorithm59 and a cosine annealing warm restarts60 learning rate scheduler, with the number of iterations set to 600 epochs. To mitigate overfitting, the training code was configured for early stopping if the model's loss on the validation set did not improve for the subsequent 10 patience epochs. Furthermore, the best weights of the model built at each iteration were retained if they achieved the minimum validation loss. Various activation functions are employed in neural networks, with the selection often influenced by the network's architecture and its predictive accuracy. In the context of multilabel classification, we opted for the Sigmoid function. This function is capable of transforming values into a range of 0 to 1 for each class, which could be defined as (z) 1 1 e = z + , which aligns well with the multilabel classification task. For training the model, we utilized the weighted binary cross-entropy loss function (L) (eq 3). This choice has shown superior performance in handling the imbalanced data set in infrared spectra signal classification when using CNN networks.27"},"pages_id":[5],"bbox":{"x0":51.333027,"y0":389.82278,"x1":292.22028,"y1":670.56104}},{"id":65,"kind":{"block_type":"TextBlock","text":"L N \nWy y y y \n1 log( ) (1 )log(1 ) i \nN i i i i i 1 \n=  + =  (3)"},"pages_id":[5],"bbox":{"x0":62.57373,"y0":679.1449,"x1":291.80692,"y1":709.9428}},{"id":66,"kind":{"block_type":"TextBlock","text":"where N is the number of classes, Wi \n, yi \n, and \ny i correspond to weight, the ground truth value, and the predicted value for class i."},"pages_id":[5],"bbox":{"x0":51.165134,"y0":720.1073,"x1":291.82666,"y1":754.2923}},{"id":67,"kind":{"block_type":"Header","text":""},"pages_id":[5],"bbox":{"x0":303.68576,"y0":40.601673,"x1":549.66785,"y1":48.741074}},{"id":68,"kind":{"block_type":"TextBlock","text":"\nTo address potential overfitting caused by the limitations of spectral signals, various data augmentation techniques were exclusively applied to the training data set. These techniques encompassed the addition of random noise within a signal-tonoise ratio (SNR) range of 2 to 20 dB (dB), random vertical shifts with a 0.3 probability, and random masking of signal portions with zeros, also with a 0.3 probability. It is crucial to highlight that none of these augmentation processes were extended to the validation and testing data sets, ensuring an unbiased evaluation of the model's performance on unseen data, thereby preserving its generalization capability."},"pages_id":[5],"bbox":{"x0":315.29694,"y0":60.567425,"x1":556.26416,"y1":179.27533}},{"id":69,"kind":{"block_type":"TextBlock","text":"In implementing DL approaches, PyTorch was the framework of choice. The hardware platform employed in this study consisted of a high-performance computer equipped with eight Intel Core i7-12700F processors running at 4.0 GHz, along with a high-speed graphics computing unit NVIDIA GeForce RTX 2060 with 12 GB of graphic memory. The networks were configured using Python 3.9 within an Anaconda environment, with PyTorch 2.0 serving as the backend for model development and training. This setup provided the necessary computational resources and software environment to conduct the experiments effectively."},"pages_id":[5],"bbox":{"x0":315.3148,"y0":181.05487,"x1":556.1515,"y1":300.10233}},{"id":70,"kind":{"block_type":"TextBlock","text":"Evaluation Metrics. Various metrics have been employed to assess the performance of the proposed DL models for functional group prediction. Accuracy serves as a comprehensive measure of the model's correctness, providing an overview of its success rate in identifying functional groups. Precision, particularly crucial in scenarios where false positives are costly, ensures the accuracy and trustworthiness of identified functional groups. In functional group prediction, recall reflects the model's effectiveness in capturing all occurrences of each functional group, thereby ensuring comprehensive coverage and preventing the oversight of critical information. The F1- score is a commonly used metric in classification tasks and it considers both precision and recall, providing a balanced measure of a model's performance. In data sets where certain functional groups are more prevalent than others, class imbalance can affect the interpretation of traditional accuracy metrics. The F1-score, being based on both precision and recall, is less sensitive to class imbalance and provides a more robust evaluation of model performance in such scenarios. Moreover, the exact match ratio (EMR) evaluates the model's precision in identifying all functional groups within a molecule, offering a strict criterion for performance assessment. EMR is particularly vital in applications necessitating precise identification of functional groups, such as drug discovery or material science. These metrics collectively contribute to the thorough evaluation of the model's efficacy in functional group prediction. The formulas of these metrics (accuracy, precision, recall, F1-score, and EMR) are as follows eqs 4−8."},"pages_id":[5],"bbox":{"x0":315.38785,"y0":303.56772,"x1":556.1694,"y1":610.0247}},{"id":71,"kind":{"block_type":"TextBlock","text":"Accuracy \nTP TN TP TN FP FN = + + + + \n(4)"},"pages_id":[5],"bbox":{"x0":327.39362,"y0":616.93146,"x1":555.703,"y1":640.4521}},{"id":72,"kind":{"block_type":"TextBlock","text":"Precision TP TP FP = + \n(5)"},"pages_id":[5],"bbox":{"x0":326.62543,"y0":653.96387,"x1":555.7204,"y1":675.9271}},{"id":73,"kind":{"block_type":"TextBlock","text":"Recall TP TP FN = + \n(6)"},"pages_id":[5],"bbox":{"x0":327.62097,"y0":689.7054,"x1":555.6496,"y1":713.1301}},{"id":74,"kind":{"block_type":"TextBlock","text":"F1 \n2 precision recall precision recall = × × + \n(7)"},"pages_id":[5],"bbox":{"x0":326.05362,"y0":726.58826,"x1":556.3966,"y1":750.87573}},{"id":75,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12363"},"pages_id":[5],"bbox":{"x0":293.34357,"y0":765.07513,"x1":555.36865,"y1":777.92804}},{"id":76,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[6],"bbox":{"x0":51.605297,"y0":37.890587,"x1":142.54112,"y1":49.527035}},{"id":77,"kind":{"block_type":"TextBlock","text":"where TP, TN, FP, FN represent the number of true positive, true negative, false positive, and false negative samples, respectively."},"pages_id":[6],"bbox":{"x0":51.388474,"y0":434.30768,"x1":290.94812,"y1":467.5232}},{"id":78,"kind":{"block_type":"TextBlock","text":"n EMR I Y Y \n1 ( ) i \nn i i 1 \n=  = =  (8)"},"pages_id":[6],"bbox":{"x0":62.83686,"y0":472.77383,"x1":291.7862,"y1":502.29007}},{"id":79,"kind":{"block_type":"TextBlock","text":"where n is the number of testing signals, Yi and \nYi are true labels and predicted labels for spectral i."},"pages_id":[6],"bbox":{"x0":51.31694,"y0":510.96786,"x1":291.4061,"y1":532.7377}},{"id":80,"kind":{"block_type":"Title","level":2,"text":" RESULTS AND DISCUSSIONS"},"pages_id":[6],"bbox":{"x0":66.19727,"y0":541.4901,"x1":197.89635,"y1":553.0782}},{"id":81,"kind":{"block_type":"TextBlock","text":"Training Results. The data set was randomly divided into three subsets: training (75%), validation (15%), and testing (10%), facilitating rigorous investigation into the training and evaluation of the proposed DL models. Following training and hyperparameter tuning, the model underwent validation over 600 epochs. A conventional IRCNN model was trained and validated in parallel with the proposed Fcg-former, enabling direct comparison within the confines of the same data set. As depicted in Figure 5, the loss function of the Fcg-former model exhibits a reliable reduction indicative of optimal convergence, whereas the IRCNN model stops training early under predefined stopping criteria. The optimal epochs for Fcgformer and IRCNN are identified at 585 and 115, respectively."},"pages_id":[6],"bbox":{"x0":51.12993,"y0":557.3797,"x1":292.1481,"y1":699.30133}},{"id":82,"kind":{"block_type":"TextBlock","text":"Prediction of Functional Groups. The ROC curve, PR curve, and the overall functional group confusion matrix are presented in Figure 6. The predictive outcomes of both IRCNN, Fcg-former, DTs, and KNN models on the testing subdata set were calculated based on the confusion matrices"},"pages_id":[6],"bbox":{"x0":51.198048,"y0":700.6655,"x1":292.491,"y1":753.66437}},{"id":83,"kind":{"block_type":"TextBlock","text":"\nand shown in Table 2. In terms of accuracy, both models demonstrate high performance, with Fcg-former slightly outperforming IRCNN by achieving an accuracy of 0.9715 compared to 0.9613. Both models also demonstrate strong precision values, indicating high accuracy in positive predictions (0.9355 versus 0.9396). However, the Fcg-former model exhibits better recall (0.9227), capturing a higher proportion of actual positive instances in the data set compared to IRCNN (0.8754). The Fcg-former model's improved recall suggests its effectiveness in capturing a broader range of functional groups within IR spectra, potentially due to its attention architecture. The F1-score, a harmonic mean of precision and recall, further confirms the overall superior performance of Fcg-former, with a score of 0.929 compared to 0.9063 for IRCNN. Additionally, Fcgformer demonstrates a higher EMR of 0.702 compared to 0.6249 for IRCNN, indicating its capability to accurately predict all functional groups within a given molecule (Figure 7)."},"pages_id":[6],"bbox":{"x0":315.10806,"y0":435.23123,"x1":556.6286,"y1":644.0053}},{"id":84,"kind":{"block_type":"TextBlock","text":"Furthermore, regarding resource management, despite Fcgformer having significantly fewer trainable parameters (6,210,065) compared to IRCNN (61,540,416), it still achieves comparable performance. Also, Fcg-former requires substantially less GPU RAM, with an estimate of 142 MB compared to 1409 MB for IRCNN, making it more memoryefficient. Notably, while IRCNN employs optimal threshold tuning for individual functional groups, resulting in enhanced accuracy evaluation, our study adopts a uniform threshold (0.5) for all functional groups. This approach highlights the"},"pages_id":[6],"bbox":{"x0":315.36508,"y0":645.3553,"x1":557.721,"y1":753.8323}},{"id":85,"kind":{"block_type":"Image","id":8,"caption":null},"pages_id":[6],"bbox":{"x0":105.617226,"y0":59.90101,"x1":513.96094,"y1":225.47267}},{"id":86,"kind":{"block_type":"Header","text":""},"pages_id":[6],"bbox":{"x0":302.48755,"y0":40.0927,"x1":549.5393,"y1":48.815025}},{"id":87,"kind":{"block_type":"Image","id":9,"caption":null},"pages_id":[6],"bbox":{"x0":94.637856,"y0":241.33055,"x1":514.56726,"y1":406.59082}},{"id":88,"kind":{"block_type":"TextBlock","text":"\nFigure 5. (A)Validation loss during training of IRCNN and Fcg-former; (B) learning rate scheduler."},"pages_id":[6],"bbox":{"x0":51.024094,"y0":413.9208,"x1":402.4154,"y1":423.9576}},{"id":89,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12364"},"pages_id":[6],"bbox":{"x0":293.30585,"y0":765.5432,"x1":555.5144,"y1":777.9057}},{"id":90,"kind":{"block_type":"TextBlock","text":"reliability of our proposed Fcg-former attention model in functional group prediction. Figures S2 and S3 show the confusion matrix of individual functional group prediction results of both models, revealing similarities to their overall performance, and confirming the enhanced performance of Fcg-former over IRCNN in predicting functional groups within IR spectra analysis."},"pages_id":[7],"bbox":{"x0":315.42258,"y0":579.5958,"x1":556.10034,"y1":660.2292}},{"id":91,"kind":{"block_type":"TextBlock","text":"The conventional machine learning techniques DTs and KNN also exhibit performance characteristics in the context of functional group prediction. The functional group confusion matrices for DTs and KNN are depicted in Figures 6E,F, S4, and S5, while their compound-level confusion matrices are illustrated in Figures 7C and 7D. DTs achieve an accuracy of 0.945, a recall of 0.8619, and a precision of 0.8625, resulting in an F1-score of 0.8622. While DTs offer a balance between"},"pages_id":[7],"bbox":{"x0":315.44272,"y0":662.06396,"x1":556.8183,"y1":753.7825}},{"id":92,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[7],"bbox":{"x0":51.59194,"y0":37.730793,"x1":142.21931,"y1":49.708397}},{"id":93,"kind":{"block_type":"Header","text":""},"pages_id":[7],"bbox":{"x0":527.3326,"y0":41.954575,"x1":549.4394,"y1":48.496162}},{"id":94,"kind":{"block_type":"Image","id":10,"caption":""},"pages_id":[7],"bbox":{"x0":91.358444,"y0":39.697357,"x1":529.6009,"y1":559.2643}},{"id":95,"kind":{"block_type":"TextBlock","text":"\nFigure 6. ROC and PR curve of (A) the IRCNN model and (B) the Fcg-former; the functional group confusion matrix of (C) the IRCNN model, (D) the Fcg-former model, (E) the DTs model, and (F) the KNN model performed on the test data set."},"pages_id":[7],"bbox":{"x0":55.99423,"y0":549.22406,"x1":555.4485,"y1":567.6018}},{"id":96,"kind":{"block_type":"Title","level":3,"text":"Table 1. Self-Attention Map Calculations"},"pages_id":[7],"bbox":{"x0":51.18115,"y0":578.9231,"x1":218.75056,"y1":589.77484}},{"id":97,"kind":{"block_type":"TextBlock","text":"input: input spectrum output: self-attention map for each patch in sequence: for each attention head:"},"pages_id":[7],"bbox":{"x0":53.07236,"y0":611.09827,"x1":138.91945,"y1":652.4658}},{"id":98,"kind":{"block_type":"TextBlock","text":"calculate query, key, and value for the current patch calculate attention scores between the current patch and all other patches"},"pages_id":[7],"bbox":{"x0":99.15652,"y0":654.1325,"x1":285.81885,"y1":683.04974}},{"id":99,"kind":{"block_type":"TextBlock","text":"apply softmax to obtain attention weights store attention weights for each patch"},"pages_id":[7],"bbox":{"x0":96.86636,"y0":683.87604,"x1":228.75569,"y1":703.3955}},{"id":100,"kind":{"block_type":"TextBlock","text":"calculate mean attention scores across all attention heads represent mean attention scores as a color map"},"pages_id":[7],"bbox":{"x0":53.423775,"y0":705.99164,"x1":232.69312,"y1":725.5121}},{"id":101,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12365"},"pages_id":[7],"bbox":{"x0":293.27115,"y0":765.4807,"x1":555.63116,"y1":777.9058}},{"id":102,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[8],"bbox":{"x0":51.05393,"y0":37.965363,"x1":143.54181,"y1":49.563942}},{"id":103,"kind":{"block_type":"Image","id":11,"caption":""},"pages_id":[8],"bbox":{"x0":62.82106,"y0":147.71321,"x1":275.28848,"y1":320.54465}},{"id":104,"kind":{"block_type":"Image","id":12,"caption":null},"pages_id":[8],"bbox":{"x0":62.92403,"y0":324.48633,"x1":275.8555,"y1":496.67273}},{"id":105,"kind":{"block_type":"TextBlock","text":"precision and recall, their EMR is relatively lower at 0.4941, indicating moderate effectiveness in predicting all functional groups within a molecule correctly. On the other hand, KNN shows an accuracy of 0.9296, with a recall of 0.8698 and a lower precision of 0.7607, leading to an F1-score of 0.8116. The KNN model has an even lower EMR of 0.3151, suggesting it struggles more with accurately identifying the full set of functional groups. Overall, the DL models, IRCNN and Fcgformer, significantly outperformed the classical machine learning approaches, DTs, and KNN, particularly regarding the EMR. This highlights the superior ability of DL techniques to handle compound-level predictions, demonstrating a clear advantage over traditional machine learning methods in capturing the complexity of functional group identification."},"pages_id":[8],"bbox":{"x0":51.32336,"y0":534.0343,"x1":292.38873,"y1":688.2557}},{"id":106,"kind":{"block_type":"TextBlock","text":"The robustness of our models is evident in their performance on both the initial test data set and the external data set. As shown in Figures S6, S7, and Table S1, while there is a slight decrease in performance metrics when evaluated on the external data set, the Fcg-former model consistently shows better performance, indicating its stability and reduced"},"pages_id":[8],"bbox":{"x0":51.41967,"y0":690.14734,"x1":292.27927,"y1":753.8737}},{"id":107,"kind":{"block_type":"TextBlock","text":"\nlikelihood of overfitting compared to the classical machine learning models. On the initial test data set, the Fcg-former achieved the highest accuracy (0.9715) and EMR (0.702), outperforming IRCNN, DTs, and KNN. On the external data set, the Fcg-former maintained its superior performance with an accuracy of 0.9585 and an EMR of 0.6471, demonstrating its ability to generalize well to new data. The results indicate that the DL models, particularly Fcg-former, exhibit robust generalization capabilities without significant overfitting, especially in dealing with complex compound-level predictions. We believe these measures address the concern regarding data set independence and provide a comprehensive assessment of our models' performance."},"pages_id":[8],"bbox":{"x0":315.0198,"y0":534.1646,"x1":556.3659,"y1":677.26044}},{"id":108,"kind":{"block_type":"TextBlock","text":"Self-Attention Map in Functional Group Prediction. Figure 8 depicts an example of the attention map generated during the processing of IR spectra and its corresponding outputs. In the calculation process, attention scores are computed for each patch in a sequence by comparing it to all other patches. This is achieved by calculating the dot product between the Query of the current patch and the Key"},"pages_id":[8],"bbox":{"x0":315.42065,"y0":678.746,"x1":557.11005,"y1":753.6445}},{"id":109,"kind":{"block_type":"Title","level":3,"text":"\nTable 2. IRCNN, Fcg-Former, DTs, and KNN Performance Summary on Test Data"},"pages_id":[8],"bbox":{"x0":51.087204,"y0":58.965427,"x1":392.9826,"y1":69.763176}},{"id":110,"kind":{"block_type":"Image","id":13,"caption":""},"pages_id":[8],"bbox":{"x0":301.3243,"y0":312.4155,"x1":546.91864,"y1":497.36673}},{"id":111,"kind":{"block_type":"Header","text":""},"pages_id":[8],"bbox":{"x0":527.3191,"y0":41.286663,"x1":549.5722,"y1":48.72376}},{"id":112,"kind":{"block_type":"Image","id":14,"caption":null},"pages_id":[8],"bbox":{"x0":302.285,"y0":144.698,"x1":546.5763,"y1":313.14536}},{"id":113,"kind":{"block_type":"Header","text":""},"pages_id":[8],"bbox":{"x0":302.79376,"y0":40.036335,"x1":360.9297,"y1":48.847874}},{"id":114,"kind":{"block_type":"TextBlock","text":"Figure 7. Compound-level functional group confusion matrix of (A) the IRCNN model, (B) the Fcg-former model, (C) the DTs model, and (D) the KNN model performed on the test data set."},"pages_id":[8],"bbox":{"x0":51.650455,"y0":503.79904,"x1":554.6365,"y1":522.94604}},{"id":115,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12366"},"pages_id":[8],"bbox":{"x0":293.3532,"y0":765.55676,"x1":555.4579,"y1":777.87274}},{"id":116,"kind":{"block_type":"TextBlock","text":"of every other patch, followed by a softmax activation (as depicted in eq 2). These attention scores, calculated for each head, provide insight into the significance of different patches with one another (Table 1). Upon examination, the attention model demonstrates a notable focus on relevant peaks within the spectra. The attention model accurately identifies the strong absorption bands associated with the alcohol functional group, particularly at 3600 cm−1 . However, for groups such as alkane and methyl, whose absorption bands overlap within the 2800−3000 cm−1 range, the attention transformer detects these features less prominently, reflecting their weaker signals. Moreover, the prominent peak observed at 1700−1750 cm−1 , revealing ester stretching vibrations and potentially carboxylic acid groups, receives significant attention from the model."},"pages_id":[9],"bbox":{"x0":51.148067,"y0":600.9045,"x1":292.19272,"y1":753.9986}},{"id":117,"kind":{"block_type":"TextBlock","text":"\nDuring the training phase, the attention transformer algorithm learns and defines the bonding interactions among various functional groups. This acquired knowledge allows the model to predict the potential presence of specific functional groups within the unknown molecules."},"pages_id":[9],"bbox":{"x0":315.0526,"y0":600.93335,"x1":556.475,"y1":654.4989}},{"id":118,"kind":{"block_type":"TextBlock","text":"Deployment of Fcg-Former. Fcg-former is an opensource library dedicated to making strides in chemical signal research accessible to the wider machine-learning community. It offers meticulously designed FcgFormer architectures through a unified API. Fcg-former emphasizes extensibility for researchers, simplicity for practitioners, and efficiency and reliability for tasks like fine-tuning and deployment."},"pages_id":[9],"bbox":{"x0":315.14697,"y0":655.0405,"x1":557.86523,"y1":732.022}},{"id":119,"kind":{"block_type":"TextBlock","text":"Additionally, users can access the library and its associated Hugging Face application, powered by Gradio, at https://"},"pages_id":[9],"bbox":{"x0":314.792,"y0":733.54956,"x1":556.0567,"y1":754.02356}},{"id":120,"kind":{"block_type":"Header","text":""},"pages_id":[9],"bbox":{"x0":51.509975,"y0":37.89098,"x1":549.387,"y1":49.441902}},{"id":121,"kind":{"block_type":"Image","id":15,"caption":null},"pages_id":[9],"bbox":{"x0":111.75295,"y0":58.24848,"x1":495.6088,"y1":293.30545}},{"id":122,"kind":{"block_type":"Image","id":16,"caption":null},"pages_id":[9],"bbox":{"x0":93.44528,"y0":354.65338,"x1":513.0637,"y1":570.1126}},{"id":123,"kind":{"block_type":"TextBlock","text":"\nFigure 8. Self-Attention mechanism works on ethyl hydrogen fumarate compound. Each cell (patch index) in the figure reflects how attention heads distribute their attention across different parts of the input. This visualization helps understand which patches receive more focus from specific attention heads during the model's processing."},"pages_id":[9],"bbox":{"x0":51.49779,"y0":300.2671,"x1":553.8891,"y1":329.6087}},{"id":124,"kind":{"block_type":"TextBlock","text":""},"pages_id":[9],"bbox":{"x0":95.66815,"y0":341.66666,"x1":222.43274,"y1":348.80975}},{"id":125,"kind":{"block_type":"TextBlock","text":"Figure 9. Functional group prediction result performed on the web-based application."},"pages_id":[9],"bbox":{"x0":51.272816,"y0":580.19183,"x1":353.73242,"y1":588.99744}},{"id":126,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12367"},"pages_id":[9],"bbox":{"x0":293.32486,"y0":765.47064,"x1":555.4257,"y1":777.88385}},{"id":127,"kind":{"block_type":"TextBlock","text":"huggingface.co/spaces/lycaoduong/FcgFormerApp. The HuggingFace App prediction result is shown in Figure 9. Other examples of our model deployment were demonstrated in Figures S8−S11."},"pages_id":[10],"bbox":{"x0":51.183704,"y0":59.44094,"x1":292.53143,"y1":102.46787}},{"id":128,"kind":{"block_type":"Title","level":2,"text":""},"pages_id":[10],"bbox":{"x0":51.621563,"y0":37.899124,"x1":142.03685,"y1":49.710663}},{"id":129,"kind":{"block_type":"Title","level":1,"text":" CONCLUSION"},"pages_id":[10],"bbox":{"x0":67.720795,"y0":110.81908,"x1":129.64787,"y1":123.185074}},{"id":130,"kind":{"block_type":"TextBlock","text":"In conclusion, this study presents a novel approach utilizing a transformer attention model for the prediction of functional groups in FTIR spectra. Our findings underscore the importance of exploring cutting-edge DL techniques in spectroscopy, paving the way for future research avenues aimed at enhancing spectral analysis and interpretation. As the field continues to evolve, integrating transformer-based models into analytical workflows could lead to significant advancements in compound characterization and identification. Our model demonstrates better performance compared to conventional CNN architectures, both in terms of functional group prediction accuracy (0.9715 over 0.9613) and compound-level accuracy (0.702 over 0.6249). The success of our transformer attention model highlights the efficacy of self-attention mechanisms in capturing intricate spectral patterns and relationships, thus enabling more accurate predictions. Overall, this work contributes to the ongoing convergence of artificial intelligence and spectroscopic analysis, offering a robust framework for accurate and efficient functional group prediction in FTIR spectra."},"pages_id":[10],"bbox":{"x0":51.135918,"y0":127.76305,"x1":292.37766,"y1":346.97644}},{"id":131,"kind":{"block_type":"Title","level":1,"text":" ASSOCIATED CONTENT"},"pages_id":[10],"bbox":{"x0":56.714966,"y0":354.90494,"x1":174.74629,"y1":367.46503}},{"id":132,"kind":{"block_type":"Title","level":1,"text":"Data Availability Statement"},"pages_id":[10],"bbox":{"x0":51.10384,"y0":369.3059,"x1":166.23375,"y1":381.6446}},{"id":133,"kind":{"block_type":"TextBlock","text":"The data sets used in this study are available from the NIST Chemistry WebBook https://webbook.nist.gov and Emission Measurement Center Spectral Database https://www3.epa. gov/ttn/emc/ftir/refnam.html. The Python code implementation of Fcg-former, model checkpoints, and all data sets used in this study are available on GitHub at https://github.com/ lycaoduong/FcgFormer."},"pages_id":[10],"bbox":{"x0":51.367134,"y0":381.18076,"x1":292.30057,"y1":458.38773}},{"id":134,"kind":{"block_type":"Title","level":1,"text":"*sı Supporting Information"},"pages_id":[10],"bbox":{"x0":51.64261,"y0":459.29953,"x1":165.82191,"y1":471.84006}},{"id":135,"kind":{"block_type":"TextBlock","text":"The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acs.analchem.4c01622."},"pages_id":[10],"bbox":{"x0":51.599545,"y0":471.6525,"x1":293.36618,"y1":494.29492}},{"id":136,"kind":{"block_type":"TextBlock","text":"The data set distribution regarding the number of functional groups within each molecule. The IRCNN, Fcg-former, DTs, and KNN performance summary on the external data set. The confusion matrix of each functional group from IRCNN, Fcg-former, DTs, and KNN models performed on the test data set. The functional group and compound-level confusion matrix of IRCNN, Fcg-former, DTs, and KNN models performed on the external data set. Examples of model deployment. Please refer to Supporting Information for additional details (PDF)"},"pages_id":[10],"bbox":{"x0":75.36407,"y0":497.23468,"x1":293.9571,"y1":618.1466}},{"id":137,"kind":{"block_type":"Title","level":1,"text":"■  AUTHOR INFORMATION"},"pages_id":[10],"bbox":{"x0":56.5426,"y0":629.8171,"x1":178.10062,"y1":642.20056}},{"id":138,"kind":{"block_type":"Title","level":3,"text":"Corresponding Authors"},"pages_id":[10],"bbox":{"x0":52.343174,"y0":645.5187,"x1":149.48131,"y1":656.03656}},{"id":139,"kind":{"block_type":"TextBlock","text":"Byeongil Lee − Digital Healthcare Research Center and Industry 4.0 Convergence Bionics Engineering, Department of Biomedical Engineering, Pukyong National University, Busan 48513, Republic of Korea; Email: bilee@pknu.ac.kr Junghwan Oh − Smart Gym-Based Translational Research Center for Active Senior's Healthcare, Pukyong National University, Busan 48513, Republic of Korea; Digital Healthcare Research Center and Industry 4.0 Convergence Bionics Engineering, Department of Biomedical Engineering,"},"pages_id":[10],"bbox":{"x0":59.647,"y0":656.0614,"x1":293.22885,"y1":754.1404}},{"id":140,"kind":{"block_type":"Header","text":""},"pages_id":[10],"bbox":{"x0":527.34174,"y0":41.5325,"x1":549.64075,"y1":48.5435}},{"id":141,"kind":{"block_type":"TextBlock","text":"\nPukyong National University, Busan 48513, Republic of Korea; Ohlabs Corp., Busan 48513, Republic of Korea; orcid.org/0000-0002-5837-0958; Phone: +82-51-629- 5771; Email: jungoh@pknu.ac.kr; Fax: +82-51-629-5779"},"pages_id":[10],"bbox":{"x0":333.2296,"y0":59.983715,"x1":552.90015,"y1":102.75513}},{"id":142,"kind":{"block_type":"Header","text":""},"pages_id":[10],"bbox":{"x0":303.093,"y0":39.916065,"x1":358.9496,"y1":48.89852}},{"id":143,"kind":{"block_type":"Title","level":1,"text":"Authors"},"pages_id":[10],"bbox":{"x0":315.39932,"y0":108.40274,"x1":348.4345,"y1":121.18026}},{"id":144,"kind":{"block_type":"TextBlock","text":"Vu Hoang Minh Doan − Smart Gym-Based Translational Research Center for Active Senior's Healthcare, Pukyong National University, Busan 48513, Republic of Korea Cao Duong Ly − Research and Development Department, Senior AI Research Engineer, Vision-in Inc., Seoul 08505, Republic of Korea Sudip Mondal − Digital Healthcare Research Center, Pukyong National University, Busan 48513, Republic of Korea; orcid.org/0000-0002-0638-9657 Thi Thuy Truong − Industry 4.0 Convergence Bionics Engineering, Department of Biomedical Engineering, Pukyong National University, Busan 48513, Republic of Korea Tan Dung Nguyen − Industry 4.0 Convergence Bionics Engineering, Department of Biomedical Engineering, Pukyong National University, Busan 48513, Republic of Korea Jaeyeop Choi − Smart Gym-Based Translational Research Center for Active Senior's Healthcare, Pukyong National University, Busan 48513, Republic of Korea"},"pages_id":[10],"bbox":{"x0":324.19226,"y0":122.143585,"x1":558.6661,"y1":320.45367}},{"id":145,"kind":{"block_type":"TextBlock","text":"Complete contact information is available at: https://pubs.acs.org/10.1021/acs.analchem.4c01622"},"pages_id":[10],"bbox":{"x0":315.4538,"y0":323.55664,"x1":518.96515,"y1":345.26447}},{"id":146,"kind":{"block_type":"Title","level":2,"text":"Author Contributions "},"pages_id":[10],"bbox":{"x0":314.8116,"y0":355.5209,"x1":404.7327,"y1":367.3278}},{"id":147,"kind":{"block_type":"TextBlock","text":"# \nV.H.M.D., C.D.L. and S.M. contributed equally to this work."},"pages_id":[10],"bbox":{"x0":315.54233,"y0":367.071,"x1":556.8646,"y1":377.9174}},{"id":148,"kind":{"block_type":"Title","level":3,"text":"Notes"},"pages_id":[10],"bbox":{"x0":315.35434,"y0":380.2892,"x1":339.77332,"y1":391.20123}},{"id":149,"kind":{"block_type":"TextBlock","text":"The authors declare no competing financial interest."},"pages_id":[10],"bbox":{"x0":315.13904,"y0":391.50882,"x1":519.6701,"y1":402.18918}},{"id":150,"kind":{"block_type":"Title","level":2,"text":" ACKNOWLEDGMENTS"},"pages_id":[10],"bbox":{"x0":329.70575,"y0":412.0326,"x1":432.2656,"y1":424.11304}},{"id":151,"kind":{"block_type":"TextBlock","text":"This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (no. 2022R1A5A8023404)."},"pages_id":[10],"bbox":{"x0":315.1748,"y0":427.7881,"x1":557.4858,"y1":459.87146}},{"id":152,"kind":{"block_type":"Title","level":2,"text":" REFERENCES"},"pages_id":[10],"bbox":{"x0":332.04532,"y0":469.55298,"x1":389.729,"y1":481.21658}},{"id":153,"kind":{"block_type":"TextBlock","text":"(1) Struyf, J. J. J. Chem. Educ. 2011, 88 (7), 937−943."},"pages_id":[10],"bbox":{"x0":322.82828,"y0":485.5179,"x1":509.7956,"y1":493.88828}},{"id":154,"kind":{"block_type":"TextBlock","text":"(2) Hong, S.; Shen, X.-J.; Xue, Z.; Sun, Z.; Yuan, T. Q. Green Chem. 2020, 22 (21), 7219−7232."},"pages_id":[10],"bbox":{"x0":315.44733,"y0":495.54788,"x1":555.8937,"y1":514.2257}},{"id":155,"kind":{"block_type":"TextBlock","text":"(3) Naqvi, S. T. R.; Rasheed, T.; Hussain, D.; ul Haq, M. N.; Majeed, S.; Ahmed, N.; Nawaz, R. J. J. Mol. Liq. 2020, 297, 111919."},"pages_id":[10],"bbox":{"x0":315.43515,"y0":514.85583,"x1":556.73785,"y1":534.07434}},{"id":156,"kind":{"block_type":"TextBlock","text":"(4) Rothfuss, N. E.; Petters, M. D. Environ. Sci. Technol. 2017, 51 (1), 271−279."},"pages_id":[10],"bbox":{"x0":316.3847,"y0":535.44434,"x1":555.79065,"y1":554.0034}},{"id":157,"kind":{"block_type":"TextBlock","text":"(5) Pellenz, L.; de Oliveira, C. R. S.; da Silva Junior, ́ A. H.; da Silva, L. J. S.; da Silva, L.; de Souza, A. A. U.; de Souza, S. M. d. A. G. U.; Borba, F. H.; da Silva, A. Sep. Purif. Technol. 2023, 305, 122435."},"pages_id":[10],"bbox":{"x0":314.815,"y0":555.29065,"x1":556.4265,"y1":583.92993}},{"id":158,"kind":{"block_type":"TextBlock","text":"(6) Ertl, P.; Altmann, E.; McKenna, J. M. J. Med. Chem. 2020, 63 (15), 8408−8418."},"pages_id":[10],"bbox":{"x0":316.26657,"y0":585.54395,"x1":556.723,"y1":603.7579}},{"id":159,"kind":{"block_type":"TextBlock","text":"(7) Wang, Y.; Wu, C. Biomacromolecules 2018, 19 (6), 1804−1825."},"pages_id":[10],"bbox":{"x0":318.06845,"y0":605.01215,"x1":556.7544,"y1":613.9428}},{"id":160,"kind":{"block_type":"TextBlock","text":"(8) Perovic, M.; Qin, Q.; Oschatz, M. Adv. Funct. Mater. 2020, 30 (41), 1908371."},"pages_id":[10],"bbox":{"x0":317.40024,"y0":614.9979,"x1":556.61816,"y1":633.84296}},{"id":161,"kind":{"block_type":"TextBlock","text":"(9) George, A.; Sanjay, M.; Srisuk, R.; Parameswaranpillai, J.; Siengchin, S. Int. J. Biol. Macromol. 2020, 154, 329−338."},"pages_id":[10],"bbox":{"x0":316.09128,"y0":635.34845,"x1":556.3176,"y1":654.1301}},{"id":162,"kind":{"block_type":"TextBlock","text":"(10) Bhong, M.; Khan, T. K.; Devade, K.; Krishna, B. V.; Sura, S.; Eftikhaar, H.; Thethi, H. P.; Gupta, N. Mater. Today: Proc. 2023."},"pages_id":[10],"bbox":{"x0":315.33667,"y0":655.14606,"x1":556.0458,"y1":673.80566}},{"id":163,"kind":{"block_type":"TextBlock","text":"(11) Mäder, P.; Kattner, L. J. J. Med. Chem. 2020, 63 (23), 14243− 14275."},"pages_id":[10],"bbox":{"x0":316.694,"y0":675.382,"x1":556.2274,"y1":693.94727}},{"id":164,"kind":{"block_type":"TextBlock","text":"(12) Baker, M. J.; Trevisan, J.; Bassan, P.; Bhargava, R.; Butler, H. J.; Dorling, K. M.; Fielden, P. R.; Fogarty, S. W.; Fullwood, N. J.; Heys, K. A.; et al. Nat. Protoc. 2014, 9 (8), 1771−1791."},"pages_id":[10],"bbox":{"x0":315.29614,"y0":695.9061,"x1":556.6285,"y1":723.6936}},{"id":165,"kind":{"block_type":"TextBlock","text":"(13) Dong, Y.; Zhang, X.; Chen, L.; Meng, W.; Wang, C.; Cheng, Z.; Liang, H.; Wang, F. Renew. Sustain. Energy Rev. 2023, 188, 113801. (14) Rhodes, C. J.; Macrae, R. M. Sci. Prog. 2015, 98 (1), 12−33."},"pages_id":[10],"bbox":{"x0":315.464,"y0":725.4822,"x1":556.34235,"y1":753.6041}},{"id":166,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12368"},"pages_id":[10],"bbox":{"x0":293.46283,"y0":765.46027,"x1":555.85754,"y1":777.9205}},{"id":167,"kind":{"block_type":"Title","level":3,"text":""},"pages_id":[11],"bbox":{"x0":51.84119,"y0":39.585102,"x1":141.75446,"y1":49.383842}},{"id":168,"kind":{"block_type":"TextBlock","text":"(15) Smith, B. C. Infrared Spectral Interpretation: A Systematic Approach; CRC Press, 2018."},"pages_id":[11],"bbox":{"x0":52.200535,"y0":59.243546,"x1":291.99112,"y1":78.45974}},{"id":169,"kind":{"block_type":"TextBlock","text":"(16) Cremer, D.; Kraka, E. Curr. Org. Chem. 2010, 14 (15), 1524− 1560."},"pages_id":[11],"bbox":{"x0":52.118164,"y0":79.65807,"x1":292.07056,"y1":98.13885}},{"id":170,"kind":{"block_type":"TextBlock","text":"(17) Bacsik, Z.; Mink, J.; Keresztury, G. Appl. Spectrosc. Rev. 2004, 39 (3), 295−363."},"pages_id":[11],"bbox":{"x0":52.20538,"y0":100.37642,"x1":293.23077,"y1":119.478745}},{"id":171,"kind":{"block_type":"TextBlock","text":"(18) Fine, J. A.; Rajasekar, A. A.; Jethava, K. P.; Chopra, G. Chem. Sci. 2020, 11 (18), 4618−4630."},"pages_id":[11],"bbox":{"x0":51.9671,"y0":121.066536,"x1":292.40692,"y1":139.46992}},{"id":172,"kind":{"block_type":"TextBlock","text":"(19) Wang, T.; Tan, Y.; Chen, Y. Z.; Tan, C. J. Chem. Inf. Model. 2023, 63 (15), 4615−4622."},"pages_id":[11],"bbox":{"x0":51.6158,"y0":141.3103,"x1":292.2206,"y1":159.64221}},{"id":173,"kind":{"block_type":"TextBlock","text":"(20) Acquarelli, J.; van Laarhoven, T.; Gerretzen, J.; Tran, T. N.; Buydens, L. M.; Marchiori, E. Anal. Chim. Acta 2017, 954, 22−31."},"pages_id":[11],"bbox":{"x0":52.14802,"y0":161.71211,"x1":292.12292,"y1":180.1673}},{"id":174,"kind":{"block_type":"TextBlock","text":"(21) Yuanyuan, C.; Zhibin, W. Chemom. Intell. Lab. Syst. 2018, 181, 1−10."},"pages_id":[11],"bbox":{"x0":51.97944,"y0":181.99547,"x1":291.8283,"y1":199.93448}},{"id":175,"kind":{"block_type":"TextBlock","text":"(22) Ng, W.; Minasny, B.; Montazerolghaem, M.; Padarian, J.; Ferguson, R.; Bailey, S.; McBratney, A. B. Geoderma 2019, 352, 251− 267."},"pages_id":[11],"bbox":{"x0":51.72803,"y0":202.49947,"x1":292.1248,"y1":231.54472}},{"id":176,"kind":{"block_type":"TextBlock","text":"(23) Chen, Y. Y.; Wang, Z. B. J. Chemom. 2019, 33 (5), No. e3122. (24) McCarthy, M.; Lee, K. L. K. J. Phys. Chem. A 2020, 124 (15), 3002−3017."},"pages_id":[11],"bbox":{"x0":52.329952,"y0":233.73184,"x1":292.01892,"y1":261.20953}},{"id":177,"kind":{"block_type":"TextBlock","text":"(25) Enders, A. A.; North, N. M.; Fensore, C. M.; Velez-Alvarez, J.; Allen, H. C. Anal. Chem. 2021, 93 (28), 9711−9718."},"pages_id":[11],"bbox":{"x0":51.751923,"y0":264.5129,"x1":292.077,"y1":282.58218}},{"id":178,"kind":{"block_type":"TextBlock","text":"(26) Awotunde, O.; Roseboom, N.; Cai, J.; Hayes, K.; Rajane, R.; Chen, R.; Yusuf, A.; Lieberman, M. Anal. Chem. 2022, 94 (37), 12586−12594."},"pages_id":[11],"bbox":{"x0":51.388092,"y0":284.46262,"x1":292.4504,"y1":312.66858}},{"id":179,"kind":{"block_type":"TextBlock","text":"(27) Jung, G.; Jung, S. G.; Cole, J. M. Chem. Sci. 2023, 14 (13), 3600−3609."},"pages_id":[11],"bbox":{"x0":52.01839,"y0":314.95078,"x1":291.9393,"y1":333.14932}},{"id":180,"kind":{"block_type":"TextBlock","text":"(28) Lin, T.; Wang, Y.; Liu, X.; Qiu, X. AI Open 2022, 3, 111−132. (29) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; Polosukhin, I. arXiv 2017, arXiv:1706.03762."},"pages_id":[11],"bbox":{"x0":51.551258,"y0":335.35153,"x1":293.39233,"y1":374.29318}},{"id":181,"kind":{"block_type":"TextBlock","text":"(30) Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grangier, D.; Auli, M. arXiv 2019, arXiv:1904.01038."},"pages_id":[11],"bbox":{"x0":52.610027,"y0":376.67267,"x1":292.60123,"y1":395.1995}},{"id":182,"kind":{"block_type":"TextBlock","text":"(31) Wang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; Chao, L. S. arXiv 2019, arXiv:1906.01787."},"pages_id":[11],"bbox":{"x0":51.874775,"y0":397.0395,"x1":292.3418,"y1":415.48215}},{"id":183,"kind":{"block_type":"TextBlock","text":"(32) Li, S.; Jin, X.; Xuan, Y.; Zhou, X.; Chen, W.; Wang, Y.-X.; Yan, X. arXiv 2019, arXiv:1907.00235."},"pages_id":[11],"bbox":{"x0":51.467884,"y0":417.65067,"x1":292.19992,"y1":435.79013}},{"id":184,"kind":{"block_type":"TextBlock","text":"(33) Wu, N.; Green, B.; Ben, X.; O'Banion, S. arXiv 2020, arXiv:2001.08317."},"pages_id":[11],"bbox":{"x0":51.40673,"y0":437.6447,"x1":292.28473,"y1":455.95108}},{"id":185,"kind":{"block_type":"TextBlock","text":"(34) Zhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.; Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, 2021, pp 11106−11115.."},"pages_id":[11],"bbox":{"x0":51.139473,"y0":458.16724,"x1":292.76562,"y1":497.92392}},{"id":186,"kind":{"block_type":"TextBlock","text":"(35) Jin, C.-c.; Chen, X. Expert Syst. Appl. 2021, 171, 114570."},"pages_id":[11],"bbox":{"x0":54.850414,"y0":499.52127,"x1":282.09808,"y1":508.1792}},{"id":187,"kind":{"block_type":"TextBlock","text":"(36) Che, C.; Zhang, P.; Zhu, M.; Qu, Y.; Jin, B. BMC Med. Inf. Decis. Making 2021, 21 (1), 184."},"pages_id":[11],"bbox":{"x0":52.245144,"y0":509.55746,"x1":292.0098,"y1":528.2479}},{"id":188,"kind":{"block_type":"TextBlock","text":"(37) Sun, J.; Xie, J.; Zhou, H. EEG classification with transformerbased models. In 2021 IEEE 3rd Global Conference On Life Sciences And Technologies (Lifetech); IEEE, 2021, pp 92−93."},"pages_id":[11],"bbox":{"x0":51.33959,"y0":530.30194,"x1":292.51138,"y1":558.7748}},{"id":189,"kind":{"block_type":"TextBlock","text":"(38) Cai, J.; Gan, F.; Cao, X.; Liu, W. IEEE Trans. Cogn. Commun. Netw. 2022, 8 (3), 1348−1357."},"pages_id":[11],"bbox":{"x0":51.688812,"y0":561.0938,"x1":292.20435,"y1":579.8558}},{"id":190,"kind":{"block_type":"TextBlock","text":"(39) Xue, R.; Bai, X.; Cao, X.; Zhou, F. IEEE Trans. Geosci. Rem. Sens. 2022, 60, 5111411."},"pages_id":[11],"bbox":{"x0":51.874462,"y0":581.3127,"x1":291.40335,"y1":600.1701}},{"id":191,"kind":{"block_type":"TextBlock","text":"(40) Grossberg, S. Scholarpedia 2013, 8 (2), 1888."},"pages_id":[11],"bbox":{"x0":54.998924,"y0":601.5342,"x1":233.8272,"y1":609.7177}},{"id":192,"kind":{"block_type":"TextBlock","text":"(41) Graves, A. Long short-term memory. In Studies in Computational Intelligence; Springer, 2012; pp 37−45.."},"pages_id":[11],"bbox":{"x0":53.195694,"y0":611.9496,"x1":291.44254,"y1":630.58936}},{"id":193,"kind":{"block_type":"TextBlock","text":"(42) Lin, S.; Runger, G. C. IEEE Transact. Neural Networks Learn. Syst. 2018, 29 (10), 4709−4718."},"pages_id":[11],"bbox":{"x0":51.542282,"y0":632.1384,"x1":291.74878,"y1":650.5118}},{"id":194,"kind":{"block_type":"TextBlock","text":"(43) Cueva, C. J.; Wei, X.-X. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. In International Conference on Learning Representations (ICLR), 2018. (44) Agar, J. C.; Naul, B.; Pandya, S.; van Der Walt, S.; Maher, J.; Ren, Y.; Chen, L.-Q.; Kalinin, S. V.; Vasudevan, R. K.; Cao, Y.; et al. Nat. Commun. 2019, 10 (1), 4809."},"pages_id":[11],"bbox":{"x0":51.527462,"y0":652.3129,"x1":292.60385,"y1":712.47687}},{"id":195,"kind":{"block_type":"TextBlock","text":"(45) Tang, W.; Long, G.; Liu, L.; Zhou, T.; Jiang, J.; Blumenstein, M. Rethinking 1d-cnn for time series classification: A stronger baseline. In The Tenth International Conference on Learning Representations (ICLR 2022), 2020, pp 1−7."},"pages_id":[11],"bbox":{"x0":51.168953,"y0":714.1679,"x1":292.7987,"y1":753.8171}},{"id":196,"kind":{"block_type":"Header","text":""},"pages_id":[11],"bbox":{"x0":303.59558,"y0":40.83912,"x1":549.50183,"y1":48.720863}},{"id":197,"kind":{"block_type":"TextBlock","text":"\n(46) Yu, G.; Ma, B.; Chen, J.; Li, X.; Li, Y.; Li, C. J. Food Process. Eng. 2021, 44 (1), No. e13602."},"pages_id":[11],"bbox":{"x0":315.2543,"y0":58.952576,"x1":556.43243,"y1":78.014435}},{"id":198,"kind":{"block_type":"TextBlock","text":"(47) Yoo, S.-H.; Huang, G.; Hong, K.-S. Bioengineering 2023, 10 (6), 685."},"pages_id":[11],"bbox":{"x0":315.78455,"y0":79.12961,"x1":556.84924,"y1":97.67745}},{"id":199,"kind":{"block_type":"TextBlock","text":"(48) Stein, S.; Linstrom, P.; Mallard, W.; Gaithersburg, T. NIST Chemistry WebBook; NIST Standard Reference Database Number 69, 2005, p 20899.."},"pages_id":[11],"bbox":{"x0":315.5661,"y0":98.9317,"x1":556.96783,"y1":127.72815}},{"id":200,"kind":{"block_type":"TextBlock","text":"(49) Kim, S.; Thiessen, P. A.; Bolton, E. E.; Chen, J.; Fu, G.; Gindulyte, A.; Han, L.; He, J.; He, S.; Shoemaker, B. A.; et al. Nucleic Acids Res. 2016, 44 (D1), D1202−D1213."},"pages_id":[11],"bbox":{"x0":315.13232,"y0":129.38097,"x1":556.50543,"y1":157.82654}},{"id":201,"kind":{"block_type":"TextBlock","text":"(50) Landrum, G. RDKit: Open-Source Cheminformatics, 2006."},"pages_id":[11],"bbox":{"x0":317.7435,"y0":158.80363,"x1":537.9429,"y1":167.72958}},{"id":202,"kind":{"block_type":"TextBlock","text":"(51) Pala, A.; Oleynik, A.; Utseth, I.; Handegard, N. O. ICES J. Mar. Sci. 2023, 80 (10), 2530−2544."},"pages_id":[11],"bbox":{"x0":315.56204,"y0":169.27313,"x1":556.3315,"y1":188.02933}},{"id":203,"kind":{"block_type":"TextBlock","text":"(52) Niu, Z.; Zhong, G.; Yu, H. Neurocomputing 2021, 452, 48−62."},"pages_id":[11],"bbox":{"x0":319.67307,"y0":189.09656,"x1":556.5771,"y1":198.51414}},{"id":204,"kind":{"block_type":"TextBlock","text":"(53) Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. arXiv 2018, arXiv:1810.04805."},"pages_id":[11],"bbox":{"x0":315.51303,"y0":198.91496,"x1":555.6441,"y1":217.43678}},{"id":205,"kind":{"block_type":"TextBlock","text":"(54) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V. arXiv 2019, arXiv:1907.11692."},"pages_id":[11],"bbox":{"x0":315.33313,"y0":219.42285,"x1":556.77893,"y1":247.37302}},{"id":206,"kind":{"block_type":"TextBlock","text":"(55) Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S. arXiv 2020, arXiv:2010.11929."},"pages_id":[11],"bbox":{"x0":314.88025,"y0":249.16087,"x1":557.41516,"y1":277.81995}},{"id":207,"kind":{"block_type":"TextBlock","text":"(56) Wang, G.; Lu, Y.; Cui, L.; Lv, T.; Florencio, D.; Zhang, C. A simple yet effective learnable positional encoding method for improving document transformer model Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, 2022, pp 453− 463."},"pages_id":[11],"bbox":{"x0":314.9125,"y0":279.2395,"x1":558.06256,"y1":328.30084}},{"id":208,"kind":{"block_type":"TextBlock","text":"(57) Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; Dauphin, Y. N. Convolutional sequence to sequence learning. In International conference on machine learning; PMLR, 2017, pp 1243−1252."},"pages_id":[11],"bbox":{"x0":315.26328,"y0":329.80862,"x1":557.903,"y1":358.01562}},{"id":209,"kind":{"block_type":"TextBlock","text":"(58) Liu, X.; Wang, Y.; Ji, J.; Cheng, H.; Zhu, X.; Awa, E.; He, P.; Chen, W.; Poon, H.; Cao, G. arXiv 2020, arXiv:2002.07972."},"pages_id":[11],"bbox":{"x0":315.8961,"y0":359.02222,"x1":556.937,"y1":377.94214}},{"id":210,"kind":{"block_type":"TextBlock","text":"(59) Kingma, D. P.; Ba, J. arXiv 2014, arXiv:1412.6980."},"pages_id":[11],"bbox":{"x0":319.85895,"y0":378.9309,"x1":517.3464,"y1":387.5368}},{"id":211,"kind":{"block_type":"TextBlock","text":"(60) Loshchilov, I.; Hutter, F. arXiv 2016, arXiv:1608.03983."},"pages_id":[11],"bbox":{"x0":319.59457,"y0":388.84442,"x1":536.24506,"y1":397.5394}},{"id":212,"kind":{"block_type":"Footer","text":"https://doi.org/10.1021/acs.analchem.4c01622 Anal. Chem. 2024, 96, 12358−12369\r\n12369"},"pages_id":[11],"bbox":{"x0":293.31393,"y0":765.36584,"x1":555.6714,"y1":777.99255}}],"debug_path":null,"metadata":{"parsing_duration":2819,"ferrules_version":"0.1.7"}}